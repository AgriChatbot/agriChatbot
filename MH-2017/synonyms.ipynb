{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,os\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords \n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist, MLEProbDist\n",
    "from numpy import array\n",
    "import string\n",
    "import operator\n",
    "import copy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputques():\n",
    "    words = pd.DataFrame()\n",
    "\n",
    "\n",
    "    tempo = pd.read_csv('all_files.csv')\n",
    "    words = pd.concat([tempo,words])\n",
    "\n",
    "    questions = list(words['questions'])\n",
    "    answers = list(words['answers'])\n",
    "    district = list(words['district'])\n",
    "    state = list(words['state'])\n",
    "\n",
    "    #print words['questions']\n",
    "    #q = list(questions)\n",
    "    #print q[1]\n",
    "\n",
    "    main = []\n",
    "    for w in list(questions):\n",
    "        w=str(w)\n",
    "        main.append(w.split(' '))\n",
    "\n",
    "    #print  main[0:10]\n",
    "\n",
    "    new_words = []\n",
    "    all_words = []\n",
    "    for w,i in enumerate(main):\n",
    "        temp = []\n",
    "        for j in i:\n",
    "            if j not in stop_words:\n",
    "                temp.append(j)\n",
    "                all_words.append(j)\n",
    "\n",
    "        new_words.append(temp)\n",
    "    return [district,state,answers,new_words,all_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingSynonymCheck(new_words,all_words):\n",
    "    realwords=[]\n",
    "    for i in all_words:\n",
    "        if i.isalpha():\n",
    "            realwords.append(i)\n",
    "    \n",
    "    lemmatized_words=[]\n",
    "    for i in realwords:\n",
    "        n=lemmatizer.lemmatize(i)\n",
    "        lemmatized_words.append(str(n))   \n",
    "    \n",
    "    FreqDictionary=FreqDist(lemmatized_words)\n",
    "    l=sorted(FreqDictionary.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    \n",
    "    #print l\n",
    "    \n",
    "    unique_words=[]\n",
    "    for i in l:\n",
    "        unique_words.append(i[0])\n",
    "        \n",
    "        \n",
    "    # Taking first words from all synsets\n",
    "    # synDict={}\n",
    "    # flag=0\n",
    "    # for word in unique_words:\n",
    "    #     if word.isalpha():\n",
    "    #         syns=wn.synsets(word)\n",
    "    #         lst=[]\n",
    "    #         for s in syns:\n",
    "    #             a=s.lemmas()[0].name()\n",
    "    #             if a!=word and (a not in lst):\n",
    "    #                 lst.append(str(a))\n",
    "    #         if lst!=[]:\n",
    "    #             synDict[word]=(lst)\n",
    "        \n",
    "        \n",
    "    # Taking the first synset\n",
    "    synDict={}\n",
    "    flag=0\n",
    "    for word in unique_words:\n",
    "        if word.isalpha():\n",
    "            syns=wn.synsets(word)\n",
    "            lst=[word]\n",
    "            if syns!=[]:\n",
    "            #for s in syns:\n",
    "                s=syns[0]\n",
    "                a=s.lemmas()\n",
    "                for i in a:\n",
    "                    f=i.name()\n",
    "                    if f!=word and (f not in lst):\n",
    "                        lst.append(str(f))\n",
    "            if lst!=[]:\n",
    "                synDict[word]=(lst)\n",
    "                \n",
    "    synunique_words=copy.deepcopy(unique_words)         ## synunique_words : copy that contains all the unique words in the questions initially\n",
    "    \n",
    "    for word in unique_words:\n",
    "        i = unique_words.index(word)\n",
    "        if word in synDict.keys():\n",
    "            for syn in synDict[word]:            \n",
    "                for j in unique_words[i+1:]:\n",
    "                    if syn==j:\n",
    "                        idx=unique_words.index(j)\n",
    "                        unique_words[idx]=word\n",
    "            \n",
    "    return [unique_words,synunique_words]              # unique words : changed words , synunique_words : original words unique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Difference(new_words,all_words):\n",
    "    \n",
    "    lst=TrainingSynonymCheck(new_words,all_words)\n",
    "    \n",
    "    unique_words=lst[0]\n",
    "    synunique_words=lst[1]\n",
    "    \n",
    "    diff={}\n",
    "    for i in range(0,len(unique_words)):\n",
    "        if unique_words[i]!=synunique_words[i]:\n",
    "            diff[str(synunique_words[i])]=unique_words[i]\n",
    "    \n",
    "    #print diff\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changingQuestions(new_words,all_words):\n",
    "    \n",
    "    diff=Difference(new_words,all_words)\n",
    "        \n",
    "    changed_ques=copy.deepcopy(new_words)\n",
    "    \n",
    "    changedindexes=[]\n",
    "    for ques in changed_ques:\n",
    "        for word in ques:\n",
    "            if word in diff.keys():\n",
    "                i=changed_ques.index(ques)\n",
    "                changedindexes.append(i)\n",
    "                idx=ques.index(word)\n",
    "                ques[idx]=diff[word]\n",
    "            \n",
    "    return changed_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createcsv():\n",
    "    [district,state,answers,new_words,all_words]=inputques()\n",
    "    #[unique_words,synunique_words]=TrainingSynonymCheck(new_words,all_words)\n",
    "    #diff=Difference(unique_words,synunique_words)\n",
    "    \n",
    "    changed_ques= changingQuestions(new_words,all_words)\n",
    "    p=len(district)\n",
    "    q=len(state)\n",
    "    r=len(answers)\n",
    "    s=len(changed_ques)\n",
    "    print p,q,r,s\n",
    "    #df1=pd.DataFrame(district,state,answers,changed_ques)\n",
    "    #print df1\n",
    "    #df1.to_csv('all_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14394 14394 14394 14394\n"
     ]
    }
   ],
   "source": [
    "createcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
